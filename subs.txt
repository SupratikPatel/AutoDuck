Here are the subtitles of the video, formatted as requested:

CH GPT exploring the world on a robot did over 1 million views and got a lot of positive comments. This time, I want to **run it locally on my own computer and GPU** with open source large language models. **AI will see the words through the camera, control the motors and the speakers**. I will also try to run it on a brand new Jetson Orin Nano directly on a robot. And the drama between me and AI will get personal. Someone needs to clean up a bit, can't even see the floor. Clean up a bit, what do you mean?

In order to get there, I had to solve five key problems. And then put a mini GPU on top of this robot. Let's start with the first one. Here I have some of my robots that I collected over the years. As you can see, some of them are quite dusty, but every single one was used in a project. For example, this might look like a Roomba, but it's a Roomba made to learn robotics. This might look like an RC car, and it is an RC car, but I'm transforming it into an AI controlled robot. This robot was following a red ball. And for example, this one was racing with another robot in a racetrack using machine learning. If you want to tell me that robotics is expensive, here's the first robot I've ever built. Electronics was soldered on a cardboard attached to another piece of cardboard, few Lego bricks, two motors from toys, and that was it. It was following the light, and yes, it was working.

When it comes to the chassis for a robot that you can buy, there's plenty of option online. But be careful about the pricing because sometimes things are three or even four times cheaper. And the cheapest you can get is probably around $5 already with the motors. I will be using the **open robotic platform chassis design** according to my design rules that you can find at openroboticplatform.com. And thanks to that, I can reuse the components from my old projects and just designing the robot is way easier.

But we also need a brain for robot. Time for step two: microcontrollers. If you are just starting out, this is perfect choice for any robotics project. You can write a program, and this will basically execute all your commands.
*   **Arduino** or **Raspberry Pi Pico**.
*   Arduino is very popular, great support on the internet, a lot of examples and projects.
*   Raspberry Pi Pico is a bit newer, it is very cheap and very easy to buy, and it can be programmed in Python.
Both are great choices for simple robotics projects. Unfortunately for me, my project is a bit more complicated and these are not very useful. But I'm not going to throw them out because they will be useful in my future projects.

For that, we'll need a computer. Let me introduce you to **Raspberry Pi**. It's a single board computer, but it's a different thing from Raspberry Pi Pico that I just showed you, because that's a microcontroller and here we have a computer with RAM, with CPU, with all the connectors: USB, Ethernet, Wi-Fi, Bluetooth, HDMI, everything you might need. And most importantly, the **GPIOs that we can connect different devices to**. If you need something even smaller, there's also the zero version. After adding the Raspberry Pi, we can upload the operating system, in my case that will be Linux. We can program it, and the robot will do whatever we want it to. But we need another thing. Time for step three.

**Raspberry Pi is super cool but it is not powerful enough to run the AI model directly here**. For that, we need something more powerful: a **GPU, a graphics card**. In my previous video, the one that got so many views, I have been using OpenAI API and sending images from the robot to their servers to get the response. But there is a different approach that I would like to implement this time, and that is to **run everything locally, including the AI model**. Graphics cards these days are very powerful and not that expensive, and models get better and better and better while getting smaller and smaller and smaller. So I hope to be able to run an **LLM (Large Language Model), a multimodal version that can also accept images quite fast**. And I hope it will be faster than using the OpenAI API. In fact, to run these models, you don't even need a graphics card. So if you only have a CPU, a processor in your computer, you can try it as well. It will be very slow, but it will work, so for just experimentation, it will be just fine.

But Nikodem, running these models must be crazy complicated and require a lot of programming and knowledge and all the things. That is not true. You can get these models to run in **about 10 minutes**. It's super easy. You just download a few things from the internet and that's it. You just run your very own model locally on your computer. And I will show you how you can do it exactly step by step right now.

Search for **Ollama** and download it from the official website. You can download it for Mac OS, Linux, and Windows, and it's very easy to install, just like any other software. You can open the terminal and type `ollama`. If you see that everything is working correctly, on their website, look for the model that you are interested in. In my case, that's **LLaVA**. And you can just type `ollama pull` and name of the model, in my case `llava`. And once downloading is finished, you can just type `ollama run` and name of the model, and you can ask it questions or do whatever you want. In my case, LLaVA is multimodal, it can also accept an image at input, so I can ask questions about the image and do other stuff, which is crucial for this project. And that's it, not really that hard, right?

I will be using an **RTX 4060 to run this model**, which is a decent GPU, but it's also not that crazy expensive. So we will see how fast it will be able to process the images from the robot. I know that what I'm showing in this video might be a bit more complicated for some of you that just want to start with robotics, but no worries because if you just want to start, I have something that can help you, and that is the sponsor of this video, Skillshare. Skillshare is the largest online learning community with thousands of classes led by industry experts. And you can find your classes basically on almost anything. If you want to learn robotics, there is a great class by Mark Flohnferder that will take you from knowing nothing to building your first project. And it will do it very very quickly. You will learn not only the basics of programming Arduinos, but also the basics of using various components like potentiometers or servo mechanisms. And if you prefer programming Raspberry Pi Pico with Python or even normal computers, the class by Alvin Wan called Coding 101 Python for Beginners will be perfect for you. You will learn about the basics of Python which will be useful in robotics, data science, and web development. Recently, they also introduced the Learning paths, which is a curated and sequential collection of classes that are perfect to master a specific topic. The first 500 people to click the link in the description will get one month free trial of Skillshare. So go check out. Thanks a lot to Skillshare for sponsoring.

And now back to the video. Time for step four: that is **communication**. Because somehow we need to get an image from this robot and send it to the computer, and then get the response back to the robot. And for that, we'll use **Wi-Fi**. But Nikodem, you said everything will happen locally and we don't need an internet connection, and it will be safe. Yes, but we can connect this robot to the router and then this computer to the router, and we don't have to connect the router to the outside network, to the internet. We can have it all connected here very safely and locally.

With all of that, the robot is functional. But I would like to add another thing. Step number five: the cherry on top. To make this project really perfect, I would like to hear what AI wants to say about the world. I want to **add voice to this robot**. And since we already have the description of the image made by LLaVA, it's very easy to now convert this text to speech with tools that are called **text to speech**.
*   One of them is from Google, and it's called very originally **Google Text-to-Speech**. It's free, very easy to use, but it sounds like this: "and sitting in a room with a desk and computer screens in front of him there". It's quite robotic.
*   But fortunately, there's another solution called **ElevenLabs**. It's a Polish startup that is very good at doing text to speech. They have a lot of voices to choose from, and it sounds like this: "the camera sees a person sitting in a chair holding a camera with two computer monitors displaying code on a desk". Pretty cool, right? The only problem is that it is a paid solution. They also have a free plan, but it is very limited, so eventually you will have to pay. The good thing is that there's a lot of voices to choose from, and you can even generate your own voices or use your own voice and clone it with their software. It's very cool, I've used it multiple times for robotics projects and not only, and it works great. So if you want to check it out, there will be a link in the description. Thanks to this very rich, custom voices from ElevenLabs, we can add another thing to this robot: **personality**. So that it can sound like Sherlock Holmes: "upon gazing into the chamber verdant foliage flourishes before a vast expanse of glass admitting an ample Cascade of light". Or a happy kid that sees Lego: "wow I see a Lego box that looks super cool it's a space astronaut and it says Creator and 3 and one in on it". Or like a movie trailer because why not: "in a realm defined by the elegant dance of light and shadow the camera beholds a scene shelves brimming with the wisdom of Ages".

And now we have all of the steps for this project working but separately. We have to join them together, we have to integrate the project. And during integrating the project, you will face most of the problems. Things will break, they won't work as you would like them to, and there will be a lot of things to replace and fix. And that's the great thing about it, because it's a great opportunity to learn. In my opinion, that's the beauty of engineering, because every problem that you encounter on your way is actually not a problem, it's just an opportunity to learn and be better. So the more you integrate and the more projects you build, the easier it will be for you. Very often, it won't be easy to fix your project, in fact, it will be very, very hard, but that's totally fine, as long as you learn. Replacing screws, writing new codes, replacing components and electronics, it's all okay. Even though school told you that doing mistakes is bad. "no obstructions directly in the path to navigate further into the space". I think I learned more from my mistakes than at any school. If you persist, sooner or later, you will run out of problems to solve, and things will start to work exactly as you imagined. Code running smoothly, parts fitting perfectly, and no errors in electronics. At this point, we just need to press enter. "I've got so many boxes to sort through, it feels like I'm in a maze of boxes, let's find a way around these, I see an open space ahead, time to explore". "a room with a white cabinet against the wall no immediate obstacles or paths".

ChatGPT could easily read a title of the book, so let's see if LLaVA can do it as well. "I see a book about python in the foreground perhaps I should move back to avoid tripping over it". So so far, the robot is just driving around, nothing interesting is happening. So let's change the prompt, and by that, let's completely **change the behavior of the robot**. Now the mission is to find a robot friend. "found a person but looks like they're kneeling down next to a camera better not disturb them while they're filming let's give them space". "looks like I'm in the right place, just got to navigate this cluttered scene and we'll be friends in no time". "time to make friends with this lonely bookshelf". "person seems to be moving items in this direction maybe they need some help". I forgot to change the function for the voice, so we have been hearing the Google text to speech, and now I switched it to ElevenLabs. "they don't appear to be looking for me though robots love companionship so let's head towards that wall where humans typically hang out".

Now I'll put a few books on the ground and tell the robot to find a book about Tesla. Once again, it's super cool that **no programming is required, you just rewrite the prompt**. And you don't need any fancy algorithms to like find the text on the image and understand it, because all of that is done by the LLM. Let's see how it works. "I see several objects but none of them seem directly relevant to finding a book about Tesla I think I should move forward to have a closer look at the bookshelf in front of me". "I see boxes piled up on the floor and there's someone crouched down near them probably be careful not to disturb the person or tip over any boxes". "wait wait what's that looks like a book with sapiens written on it it's not quite what I was looking for but still interesting guess I'll just need to adjust my search parameters a bit more". "watch book on Tesla and sight".

Here is how the robot was working so far. It was taking an image, sending it for the processing on the server, getting the response, generating the speech from the text, reading out loud the text, and then taking the decision to go left, right, forward and so on. Now I want to change it a little bit and I'm curious if we skip all this speaking part and generating text and get just the decision forward, Left, Right, simple one word, will it be faster and will it be able to just drive continuously without stopping? Let's see. Each of the sprints on Terminal is a new image received from the robot, processed with the LLM, and responded to. So as you can see, it's pretty fast. It's **more than 1 image per second**, which is pretty decent considering it's not very crazy GPU that I'm using here.

The plan was to let the robot pass through the door and then go into the room, and it kind of worked but you know it's not really that perfect. Very often, the robot was just circling out, and that was it. Here we built a little maze out of boxes of filament, and as you can see, it's definitely not perfect. It does not have a **sense of scale and a sense of the environment around it**. And in open space, it looks like this: it just circles and sometimes going front. We can conclude that these models are not really curious. There is a lot of **safety features implemented**. But who knows what the future will bring, maybe once the AGI is ready, I should redo this video again.

But I am not quite done yet, I got a full. But it's not going to stop me because I also got a new toy. This is the **Jetson Orin Nano Super from Nvidia**. It's a new model that is more powerful than the previous one, and it's also a lot cheaper. It's just $249, which is a pretty good price. They claim that this thing can run LLMs directly here because it has a **GPU built on board**. So now I'm thinking that maybe I should just grab a robot, which as you can see, I already disassembled the Raspberry Pi and the camera, the speakers, and put the Jetson on top. And then connect everything and have everything run here locally without any connection to even the local server, the LLM and everything on the robot. Look at that setup, it is so small compared to the previous one. We don't have a huge bulky camera with the speakers, just a small webcam. The only problem is that this robot won't speak, there are no speakers as you can see because there is no audio jack on the Jetson Nano. I could get that to work with the USB, but I don't want to bother with that. I will miss a bit the robot talking to me. Now let's just connect everything properly. Here goes that to G&D. 1 2 3.

Here is the robot with the camera and running Jetson. And here is the screen connected to the Jetson. As you can see, it is working. LLaVA is responding here with left, right, and so on. The only problem is the **GPIOs doesn't work and I have no idea why**. In the end, I did not manage to get the Jetson to control the motors. But I was smart enough to test the Jetson as a server before disassembling the Raspberry Pi version of this robot. So here are clips from that. I'm running exactly the same script as I did on my computer, but this time I'm using Jetson instead of the RTX 4060. It takes about **3 seconds, maybe 4 seconds to process each image**. "this space is quite spacious but let's get out of here before we have to deal with clutter". Where is the clutter, just a few boxes of filament, I don't get it. "a long hallway with no visible obstacles let's go explore". This is the Jetson Orin Nano and this is my Windows computer, and I'm connected through SSH to the Raspberry Pi, just an empty. So you can run it here the robot and here you can see the image is sent to the Jetson, here it is processed, the message is sent back, and here you can see the response and everything that's going on. Also like, "I'm about to make a very very important left turn better slow down and get ready". It said left turn and then went back, I don't get it, but whatever. A lot of errors from OpenCV, I don't know why, but it just happens, but it still works. "someone needs to clean up a bit, can't even see the floor". Clean up a bit, what do you mean? Here we can see that the robot started to test my patience, so I decided to end the experiment before it too late. Thanks a lot for watching. Thanks a lot to Skillshare for sponsoring, and see you in the next one. Bye.