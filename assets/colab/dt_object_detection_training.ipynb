{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mIJd6b6pbsk"
      },
      "source": [
        "# First, let us set up a few dependencies\n",
        "\n",
        "Don't forget to switch to a GPU-enabled colab runtime!\n",
        "\n",
        "```\n",
        "Runtime -> Change Runtime Type -> GPU\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E2APl4pnbAa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import contextlib\n",
        "import subprocess\n",
        "import tempfile # Added here as it's used before SESSION_WORKSPACE cell in .py\n",
        "import shutil   # Added here as it's used before SESSION_WORKSPACE cell in .py\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def directory(name):\n",
        "  ret = os.getcwd()\n",
        "  os.chdir(name)\n",
        "  yield None\n",
        "  os.chdir(ret)\n",
        "\n",
        "def run(command, exception_on_failure=False):\n",
        "  try:\n",
        "    program_output = subprocess.check_output(f\"{command}\", shell=True, universal_newlines=True, stderr=subprocess.STDOUT)\n",
        "  except Exception as e:\n",
        "    if exception_on_failure:\n",
        "      raise e\n",
        "    program_output = e.output\n",
        "    print(f\"Error in run: {program_output}\") # Added for visibility of errors\n",
        "  return program_output\n",
        "\n",
        "def prun(command, exception_on_failure=False):\n",
        "  x = run(command, exception_on_failure)\n",
        "  print(x)\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6gJLjcipgNw"
      },
      "source": [
        "# This mounts your google drive to this notebook. You might have to change the path to fit with your dataset folder inside your drive.\n",
        "\n",
        "Read the instruction output by the cell bellow carefully!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shRmyOAbatwZ"
      },
      "outputs": [],
      "source": [
        "# Create a temporary workspace\n",
        "SESSION_WORKSPACE = tempfile.mkdtemp()\n",
        "print(f\"Session workspace created at: {SESSION_WORKSPACE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iod0c4KF_Vxo"
      },
      "outputs": [],
      "source": [
        "# Mount the drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "    print(\"Google Drive mounted.\")\n",
        "except ImportError:\n",
        "    print(\"Google Colab 'drive' not available. Assuming local execution or pre-mounted drive.\")\n",
        "    print(\"Please ensure your dataset is accessible at the specified DRIVE_PATH or modify script.\")\n",
        "    DRIVE_PATH = os.path.expanduser(\"~/GoogleDrive/My Drive\") # Example for local\n",
        "    if not os.path.exists(DRIVE_PATH):\n",
        "         print(f\"Warning: Default DRIVE_PATH {DRIVE_PATH} does not exist.\")\n",
        "         DRIVE_PATH = input(f\"Please enter the path to your dataset zip file's directory (e.g., /path/to/drive/My Drive): \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhdlb1osbf_a"
      },
      "outputs": [],
      "source": [
        "# Unzip the dataset\n",
        "# import shutil # Moved to Cell 2\n",
        "# import os # Moved to Cell 2\n",
        "\n",
        "DATASET_DIR_NAME = \"duckietown_object_detection_dataset\"\n",
        "DATASET_ZIP_NAME = f\"{DATASET_DIR_NAME}.zip\"\n",
        "DATASET_DIR_PATH = os.path.join(SESSION_WORKSPACE, DATASET_DIR_NAME)\n",
        "TRAIN_DIR = \"train\"\n",
        "VALIDATION_DIR = \"val\"\n",
        "IMAGES_DIR = \"images\"\n",
        "LABELS_DIR = \"labels\"\n",
        "\n",
        "def show_info(base_path: str):\n",
        "  for l1 in [TRAIN_DIR, VALIDATION_DIR]:\n",
        "    for l2 in [IMAGES_DIR, LABELS_DIR]:\n",
        "      p = os.path.join(base_path, l1, l2)\n",
        "      if os.path.exists(p):\n",
        "        print(f\"#Files in {l1}/{l2}: {len(os.listdir(p))}\")\n",
        "      else:\n",
        "        print(f\"#Path not found: {p}\")\n",
        "\n",
        "def unzip_dataset():\n",
        "  zip_path = os.path.join(DRIVE_PATH, DATASET_ZIP_NAME)\n",
        "  if not os.path.exists(zip_path):\n",
        "      print(f\"No zipped dataset found at {zip_path}! Please check the path and filename.\")\n",
        "      alt_zip_path = input(f\"Enter the full path to '{DATASET_ZIP_NAME}' if it's elsewhere, or press Enter to abort: \")\n",
        "      if alt_zip_path and os.path.exists(alt_zip_path):\n",
        "          zip_path = alt_zip_path\n",
        "      else:\n",
        "          print(\"Dataset zip file not found. Aborting.\")\n",
        "          return False\n",
        "\n",
        "  print(\"Unpacking zipped data...\")\n",
        "  shutil.unpack_archive(zip_path, DATASET_DIR_PATH)\n",
        "  print(f\"Zipped dataset unpacked to {DATASET_DIR_PATH}\")\n",
        "  show_info(DATASET_DIR_PATH)\n",
        "  return True\n",
        "\n",
        "if not unzip_dataset():\n",
        "    raise RuntimeError(\"Dataset unzipping failed. Cannot continue.\") # Or use pass / print\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpWozMbUeFZA"
      },
      "outputs": [],
      "source": [
        "# change working directory to the session workspace\n",
        "os.chdir(SESSION_WORKSPACE)\n",
        "print(f\"PWD: {os.getcwd()}\")\n",
        "\n",
        "# install pytorch and torchvision\n",
        "!pip3 install torch==1.13.0 torchvision==0.14.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc4VMcwmpr84"
      },
      "source": [
        "# Next, we will install YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8L68QAeZF9G"
      },
      "outputs": [],
      "source": [
        "!pip3 install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5iilV-e76YN"
      },
      "source": [
        "# We now inform the training process of the format and location of our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7u3D124u8Flw"
      },
      "outputs": [],
      "source": [
        "%%writefile duckietown.yaml\n",
        "\n",
        "# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\n",
        "train: duckietown_object_detection_dataset/train # Path relative to this YAML file (assumes CWD is SESSION_WORKSPACE)\n",
        "val: duckietown_object_detection_dataset/val   # Path relative to this YAML file (assumes CWD is SESSION_WORKSPACE)\n",
        "\n",
        "# number of classes\n",
        "nc: 4\n",
        "\n",
        "# class names\n",
        "names: [ 'duckie', 'cone', 'truck', 'bus' ]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CalmQI9Ypx5v"
      },
      "source": [
        "# And we're ready to train! This step will take about 5 minutes.\n",
        "\n",
        "Notice that we're only training for 10 epochs. That's probably not enough!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kss7Oid6OkAv"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os # Already imported, but good for cell self-containment\n",
        "\n",
        "BEST_MODEL_PATH = None\n",
        "try:\n",
        "    # Load a pretrained YOLO model (e.g., 'yolo11n.pt')\n",
        "    # Ultralytics will automatically download it if not found locally.\n",
        "    model = YOLO('yolo11n.pt') # Or 'yolov8n.pt'\n",
        "\n",
        "    # Train the model\n",
        "    results = model.train(\n",
        "        data='duckietown.yaml',        # Path to your dataset config\n",
        "        epochs=10,                     # Number of epochs\n",
        "        imgsz=416,                     # Image size\n",
        "        batch=32,                      # Batch size\n",
        "        project='runs/train',          # Project directory for saving results\n",
        "        name='exp',                    # Experiment name\n",
        "        # device can be 'cpu' or 'cuda' (or 0 for GPU). Auto-detection is default.\n",
        "    )\n",
        "\n",
        "    # Get the path to the best model\n",
        "    BEST_MODEL_PATH = os.path.join(results.save_dir, 'weights/best.pt')\n",
        "    print(f\"Best model saved to: {BEST_MODEL_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during training: {e}\")\n",
        "    BEST_MODEL_PATH = None # Ensure it's None if training failed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5jSQ4XubFqH"
      },
      "outputs": [],
      "source": [
        "if BEST_MODEL_PATH and os.path.exists(BEST_MODEL_PATH):\n",
        "    print(f\"The best model path is: {BEST_MODEL_PATH}\")\n",
        "else:\n",
        "    print(\"Training did not complete successfully, or the best model path was not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz2PZ7d0qPt0"
      },
      "source": [
        "# Next, we can upload your model to Duckietown's cloud!\n",
        "\n",
        "We will need our token to access our personal cloud space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl-HhnrFtKnp"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in the duckietown token here\n",
        "YOUR_DT_TOKEN = os.getenv(\"YOUR_DT_TOKEN\")\n",
        "if not YOUR_DT_TOKEN:\n",
        "    YOUR_DT_TOKEN = input(\"Please enter your Duckietown Token (or set YOUR_DT_TOKEN env var): \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCd7RrjLc64Z"
      },
      "source": [
        "Then, we chose the location of the trained model on disk and its name once uploaded to our cloud space. You should not change these values, or the robots will not be able to find the model to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bNXEgAFpRIH"
      },
      "outputs": [],
      "source": [
        "import os # Ensure os is imported\n",
        "\n",
        "# DO NOT CHANGE THESE\n",
        "model_name = \"yolo11n\" # Or your chosen model variant name\n",
        "model_local_path = BEST_MODEL_PATH # This will be None if training failed\n",
        "model_remote_path = f\"courses/mooc/objdet/data/nn_models/{model_name}.pt\"\n",
        "\n",
        "# install DCSS client\n",
        "!pip3 install dt-data-api\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8DYodvDdQKr"
      },
      "source": [
        "We now open a pointer to our cloud space and upload the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAU4dTNJVkyx"
      },
      "outputs": [],
      "source": [
        "import torch # Though not directly used here, often a dependency with model files\n",
        "from dt_data_api import DataClient, Storage # Import here after pip install in previous cell\n",
        "import os # Ensure os is imported\n",
        "\n",
        "if not YOUR_DT_TOKEN or YOUR_DT_TOKEN == \"YOUR_TOKEN_HERE\" or \"YOUR_TOKEN_HERE\" in YOUR_DT_TOKEN : # check for placeholder\n",
        "    print(\"Duckietown token not provided or is placeholder. Skipping model upload.\")\n",
        "else:\n",
        "    # open a pointer to our personal duckietown cloud space\n",
        "    client = DataClient(YOUR_DT_TOKEN)\n",
        "    storage = client.storage(\"user\")\n",
        "\n",
        "    # upload model\n",
        "    if model_local_path and os.path.exists(model_local_path):\n",
        "        print(f\"Uploading {model_local_path} to {model_remote_path}\")\n",
        "        upload = storage.upload(model_local_path, model_remote_path)\n",
        "        upload.join()\n",
        "        print(\"Upload complete.\")\n",
        "    else:\n",
        "        print(f\"Skipping upload: model_local_path is not valid ('{model_local_path}'). Training might have failed or model file not found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUVJ5BfBGq7F"
      },
      "source": [
        "# Done!\n",
        "\n",
        "We're done training! You can now close this tab and go back to the `Training` notebook"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
